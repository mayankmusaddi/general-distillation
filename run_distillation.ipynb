{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "!python3 train.py \\\n",
    "    --student_name xlm-roberta-base \\\n",
    "    --teacher_name roberta-base \\\n",
    "    --teacher_pretrained trained_Robertabase_128 \\\n",
    "    --alpha_ce 5.0 --alpha_mlm 2.0 --alpha_cos 1.0 --alpha_clm 0.0 --mlm \\\n",
    "    --dump_path output/train1 \\\n",
    "    --data_file data/dump.txt \\\n",
    "    --force --n_gpu 0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "07/22/2021 00:49:36 - INFO - utils - PID: 32505 -  Experiment will be dumped and logged in output/train1\n",
      "07/22/2021 00:49:36 - INFO - utils - PID: 32505 -  Param: Namespace(force=True, dump_path='output/train1', data_file='data/dump.txt', preprocessed_data_file=None, preprocessed_token_counts=None, student_name='distilroberta-base', student_pretrained=None, teacher_name='roberta-base', teacher_pretrained='trained_Robertabase_128', temperature=2.0, alpha_ce=5.0, alpha_mlm=2.0, alpha_clm=0.0, alpha_mse=0.0, alpha_cos=1.0, mlm=True, mlm_mask_prop=0.15, word_mask=0.8, word_keep=0.1, word_rand=0.1, mlm_smoothing=0.7, restrict_ce_to_mask=False, n_epoch=3, batch_size=5, group_by_size=True, gradient_accumulation_steps=50, warmup_prop=0.05, weight_decay=0.0, learning_rate=0.0005, adam_epsilon=1e-06, max_grad_norm=5.0, initializer_range=0.02, fp16=False, fp16_opt_level='O1', n_gpu=0, local_rank=0, seed=56, log_interval=500, checkpoint_interval=4000, master_port=-1, is_master=True, multi_gpu=False)\n",
      "07/22/2021 00:49:36 - INFO - utils - PID: 32505 -  Loading student from distilroberta-base\n",
      "07/22/2021 00:49:39 - INFO - utils - PID: 32505 -  Student loaded.\n",
      "07/22/2021 00:49:39 - INFO - utils - PID: 32505 -  Loading teacher from roberta-base\n",
      "07/22/2021 00:49:40 - INFO - utils - PID: 32505 -  Teacher loaded\n",
      "07/22/2021 00:49:40 - INFO - utils - PID: 32505 -  Special tokens {'bos_token': 0, 'eos_token': 2, 'unk_token': 3, 'sep_token': 2, 'pad_token': 1, 'cls_token': 0, 'mask_token': 50264}\n",
      "07/22/2021 00:49:40 - INFO - utils - PID: 32505 -  Loading data from data/dump.txt\n",
      "07/22/2021 00:49:40 - INFO - utils - PID: 32505 -  Loading Tokenizer (roberta-base)\n",
      "07/22/2021 00:49:48 - INFO - utils - PID: 32505 -  Loading text from data/dump.txt\n",
      "07/22/2021 00:49:48 - INFO - utils - PID: 32505 -  Start encoding\n",
      "07/22/2021 00:49:48 - INFO - utils - PID: 32505 -  50000 examples to process.\n",
      "07/22/2021 00:49:51 - INFO - utils - PID: 32505 -  10000 examples processed. - 2.48s/10000expl\n",
      "07/22/2021 00:49:53 - INFO - utils - PID: 32505 -  20000 examples processed. - 2.42s/10000expl\n",
      "07/22/2021 00:49:56 - INFO - utils - PID: 32505 -  30000 examples processed. - 2.37s/10000expl\n",
      "07/22/2021 00:49:58 - INFO - utils - PID: 32505 -  40000 examples processed. - 2.34s/10000expl\n",
      "07/22/2021 00:50:00 - INFO - utils - PID: 32505 -  50000 examples processed. - 2.36s/10000expl\n",
      "07/22/2021 00:50:00 - INFO - utils - PID: 32505 -  Finished binarization\n",
      "07/22/2021 00:50:00 - INFO - utils - PID: 32505 -  50000 examples processed.\n",
      "07/22/2021 00:50:00 - INFO - utils - PID: 32505 -  Dump to output/train1/roberta-base.pickle\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  Counting occurences for MLM.\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  Dump to output/train1/roberta-base.token_counts.pickle\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  Loading token counts\n",
      "/Users/mayankmusaddi/Desktop/Kdistill/ai-experiments/lm_seqs_dataset.py:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.token_ids = np.array(data)\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  Splitting 0 too long sequences.\n",
      "/Users/mayankmusaddi/Desktop/Kdistill/ai-experiments/lm_seqs_dataset.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.token_ids = np.array(new_tok_ids)\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  Remove 5275 too short (<=11 tokens) sequences.\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  Remove 0 sequences with a high level of unknown tokens (50%).\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  44725 sequences\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  Data loader created.\n",
      "07/22/2021 00:50:01 - INFO - utils - PID: 32505 -  Initializing Distiller\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  Using [0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75, 79, 83, 87, 91, 95, 99, 103, 107, 111, 115, 119, 123, 127, 131, 135, 139, 143, 147, 151, 155, 159, 163, 167, 171, 175, 179, 183, 187, 191, 195, 199, 203, 207, 211, 215, 219, 223, 227, 231, 235, 239, 243, 247, 251, 255, 259, 263, 267, 271, 275, 279, 283, 287, 291, 295, 299, 303, 307, 311, 315, 319, 323, 327, 331, 335, 339, 343, 347, 351, 355, 359, 363, 367, 371, 375, 379, 383, 387, 391, 395, 399, 403, 407, 411, 415, 419, 423, 427, 431, 435, 439, 443, 447, 451, 455, 459, 463, 467, 471, 475, 479, 483, 487, 491, 495, 499, 503, 507, 511, inf] as bins for aspect lengths quantization\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  Count of instances per bin: [4807 5641 6008 5005 5111 4324 3288 2702 2001 1621 1209  879  608  497\n",
      "  290  214  154  103   74   54   31   22   15    9   10    8    4    6\n",
      "    5    5    1    2    2    6    2    1    1    1    1    2    1]\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  Using MLM loss for LM step.\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  --- Initializing model optimizer\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  ------ Number of trainable parameters (student): 82170201\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  ------ Number of parameters (student): 82170201\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  --- Initializing Tensorboard\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  Starting training\n",
      "07/22/2021 00:50:02 - INFO - utils - PID: 32505 -  --- Starting epoch 0/2\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "-Iter:   0%|                                           | 0/8945 [00:00<?, ?it/s]\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "-Iter:   0%| | 19/8945 [00:13<1:45:44,  1.41it/s, Last_loss=12.75, Avg_cum_loss=^C\n",
      "-Iter:   0%| | 14/8945 [00:13<2:22:32,  1.04it/s, Last_loss=12.75, Avg_cum_loss=\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mayankmusaddi/Desktop/Kdistill/ai-experiments/train.py\", line 232, in <module>\n",
      "    main()\n",
      "  File \"/Users/mayankmusaddi/Desktop/Kdistill/ai-experiments/train.py\", line 227, in main\n",
      "    distiller.train()\n",
      "  File \"/Users/mayankmusaddi/Desktop/Kdistill/ai-experiments/distiller.py\", line 354, in train\n",
      "    self.step(input_ids=token_ids, attention_mask=attn_mask, lm_labels=lm_labels)\n",
      "  File \"/Users/mayankmusaddi/Desktop/Kdistill/ai-experiments/distiller.py\", line 419, in step\n",
      "    nn.functional.log_softmax(s_logits_slct / self.temperature, dim=-1),\n",
      "  File \"/Users/mayankmusaddi/.virtualenvs/kdist/lib/python3.9/site-packages/torch/nn/functional.py\", line 1605, in log_softmax\n",
      "    ret = input.log_softmax(dim)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('kdist': virtualenvwrapper)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "6ca483fc20181fb4067d79ffa5ad24861a8293ded041f675614f629756e72669"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}